# Shot Audio Generation Design

## Context
The story generation pipeline produces visual storyboards with narrative content stored in `storyboard_payload.audioAndNarrative[]`. Each entry contains type ("monologue"/"dialogue"), source (character_id or "narrator"), and line (text with performance notes). To enable voice preview experiences, we need to generate speech audio for each shot using Gemini's multi-speaker TTS API while respecting voice profiles defined in the audio design document.

## Goals / Non-Goals

### Goals
- Generate WAV audio files for shot narratives using Gemini TTS API
- Support single-speaker and multi-speaker modes based on shot content
- Store audio file paths in the database for retrieval by the UI
- Provide resume and override modes for flexible regeneration workflows
- Support both batch (entire story) and single-shot generation modes
- Reuse existing CLI flag patterns (--resume, --override as boolean flag)

### Non-Goals
- Audio editing or post-processing beyond Gemini TTS output
- Streaming audio generation or real-time preview
- Audio format conversion (WAV is sufficient for web playback)
- Custom voice training or fine-tuning (use Gemini prebuilt voices)
- Multi-pass audio generation with director feedback loops

## Decisions

### Speaker Mode Selection
Use shot's `audioAndNarrative[]` array to determine speaker mode:
- **Single-speaker mode**: Shot has only one unique source value (narrator or one character)
- **Multi-speaker mode**: Shot has exactly two unique source values
- **Validation error**: Shot has three or more unique source values (not supported by Gemini multi-speaker API)

Narrator counts as a speaker. If a shot has narrator + 2 characters, that's 3 speakers and will fail validation.

### Prompt Assembly Strategy
Assemble Gemini TTS prompt from:
1. Filter `character_voice_profiles[]` to include only characters present in the shot (matching `audioAndNarrative[].source` values)
2. Include `narrator_voice_profile` if any line has `source: "narrator"`
3. Include entire `audioAndNarrative[]` array as the script to be spoken
The final prompt will just be the concat of the 3 json objects, do not add additional instruction.

This filtered approach avoids confusing the AI model with irrelevant character profiles while ensuring all necessary voice configurations are present.

### Speaker Configuration Mapping
For Gemini TTS API:
- **Speaker name**: Use `character_id` from audio design document (matches `audioAndNarrative[].source` for exact matching)
- **Voice name**: Use `voice_name` field from the matching profile's voice config (e.g., "Kore", "Puck")
- **Narrator**: Use `source: "narrator"` as speaker name, `narrator_voice_profile.voice_name` as voice
- **Model**: Use `"gemini-2.5-flash-preview-tts"` specifically for audio generation (distinct from text/image models)

Using character_id as speaker name ensures exact matching between the shot's source field and the voice configuration, avoiding ambiguity.

The audio design document generated by the create_audio_design.md system prompt now includes `character_id` alongside `character_name` for each character voice profile. The `character_id` is generated during audio design response validation using the `normalizeNameToId` utility (from `visual-design/utils.ts`), which converts character names to kebab-case slugs (e.g., "Rhea the Explorer" â†’ "rhea-the-explorer"). Unlike the visual reference validation which removes the `character_name` field, we keep both `character_id` and `character_name` in the audio design document for clarity.

**Gemini TTS API Reference Examples** (for implementation reference, not direct copy):

Single-speaker mode:
```javascript
import {GoogleGenAI} from '@google/genai';
import wav from 'wav';

async function saveWaveFile(filename, pcmData, channels = 1, rate = 24000, sampleWidth = 2) {
  return new Promise((resolve, reject) => {
    const writer = new wav.FileWriter(filename, { channels, sampleRate: rate, bitDepth: sampleWidth * 8 });
    writer.on('finish', resolve);
    writer.on('error', reject);
    writer.write(pcmData);
    writer.end();
  });
}

const ai = new GoogleGenAI({});
const response = await ai.models.generateContent({
  model: "gemini-2.5-flash-preview-tts",
  contents: [{ parts: [{ text: 'Say cheerfully: Have a wonderful day!' }] }],
  config: {
    responseModalities: ['AUDIO'],
    speechConfig: {
      voiceConfig: {
        prebuiltVoiceConfig: { voiceName: 'Kore' },
      },
    },
  },
});

const data = response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
const audioBuffer = Buffer.from(data, 'base64');
await saveWaveFile('out.wav', audioBuffer);
```

Multi-speaker mode:
```javascript
const prompt = `TTS the following conversation between Joe and Jane:
  Joe: How's it going today Jane?
  Jane: Not too bad, how about you?`;

const response = await ai.models.generateContent({
  model: "gemini-2.5-flash-preview-tts",
  contents: [{ parts: [{ text: prompt }] }],
  config: {
    responseModalities: ['AUDIO'],
    speechConfig: {
      multiSpeakerVoiceConfig: {
        speakerVoiceConfigs: [
          {
            speaker: 'Joe',
            voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Kore' } }
          },
          {
            speaker: 'Jane',
            voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Puck' } }
          }
        ]
      }
    }
  }
});
```

### File Storage Strategy
Store audio files in the Next.js public directory for direct serving:
- Path pattern: `apps/story-tree-ui/public/generated/<story-id>/shots/<scenelet-id>/<shot-index>_audio.wav`
- Database stores relative path: `generated/<story-id>/shots/<scenelet-id>/<shot-index>_audio.wav`
- UI accesses via: `/<audio_file_path>` (public directory is root)

WAV format is chosen because:
- Direct output format from Gemini TTS API (no conversion needed)
- Native browser support without additional codecs
- Acceptable file size for short shot narrations (typically 5-30 seconds)

### Generation Mode Logic
Three modes control regeneration behavior:

1. **Default mode** (no flags): Stop immediately if ANY shot in the target scope already has `audio_file_path` set in the database. This prevents accidental overwrites.

2. **Resume mode** (`--resume` flag): Skip shots that already have `audio_file_path` set, generate audio for remaining shots. Used for continuing interrupted batch generations or adding audio to newly created shots.

3. **Override mode** (`--override` flag): Regenerate audio for ALL shots in the target scope, replacing existing files. Used when voice profiles or narrative content have changed.

For single-shot mode (--scenelet-id + --shot-index), default mode acts as a guard, resume mode is equivalent to skip-if-exists, and override mode forces regeneration.

### CLI Flag Design
Reuse existing flag patterns:
- `--resume`: Boolean flag (just `--resume`, not `--resume true`)
- `--override`: Boolean flag (just `--override`, not `--override true`)
- `--scenelet-id <id>`: Target specific scenelet for single or scenelet-batch mode
- `--shot-index <number>`: Combined with --scenelet-id for single-shot mode (1-based index)
- `--verbose` (or `-v`): Enable detailed logging of Gemini API requests

If neither --scenelet-id nor --shot-index provided: batch mode for entire story.
If only --scenelet-id provided: batch mode for that scenelet.
If both --scenelet-id and --shot-index provided: single-shot mode.

### Verbose Mode Logging
When `--verbose` flag is enabled, the audio generation task should log:
- **Request details**: All fields of the Gemini API request including model name, prompt text, speaker configurations, and voice names
- **Response metadata**: Status, content type, but NOT the binary audio data itself
- **Redaction**: Audio binary data in responses must be redacted or truncated (e.g., show only first 50 bytes as hex) to avoid polluting logs with base64-encoded audio

This helps debugging voice profile mapping issues and verifying correct API parameters without overwhelming logs with large binary payloads.

## Alternatives Considered

### Alternative: Store audio in cloud storage (S3/Supabase Storage)
**Rejected** because:
- Adds complexity and dependencies (AWS SDK, Supabase Storage API)
- Requires signed URLs or public buckets for browser access
- Local filesystem is simpler for MVP and sufficient for development
- Can migrate to cloud storage later if needed without changing database schema (just update file paths)

### Alternative: Support 3+ speakers by chaining multiple API calls
**Rejected** because:
- Adds significant complexity to handle audio splicing and timing synchronization
- Gemini API may not preserve consistent speaker characteristics across separate calls
- Current storyboard design typically has 1-2 speakers per shot; 3+ is rare
- Can be added later if user data shows need for more speakers

### Alternative: Use character_name instead of character_id for speaker mapping
**Rejected** because:
- character_name may have spaces or special characters that complicate API calls
- character_id is guaranteed unique and stable across document versions
- character_id matches the source field in audioAndNarrative exactly (no lookup needed)
- More consistent with existing design document conventions

## Risks / Trade-offs

### Risk: Audio generation latency for batch operations
**Mitigation**: Batch mode processes shots sequentially, so large stories (50+ shots) may take several minutes. Consider adding progress logging and allowing resume mode to recover from interruptions. Future optimization could add parallel processing with concurrency limits.

### Risk: Gemini TTS API may not perfectly match speaker to character_id
**Mitigation**: The API examples show speaker names as strings (e.g., "Joe", "Jane"). Using character_id (e.g., "rhea-engineer") should work similarly. If Gemini struggles with kebab-case IDs, we can add a character name normalization step, but initial implementation uses character_id directly for consistency.

### Trade-off: WAV file size vs. audio quality
**Mitigation**: WAV files are larger than compressed formats (MP3, Opus) but avoid transcoding complexity and quality loss. For short shot narrations (5-30 seconds), file sizes are manageable (typically 200KB-1MB per shot). Future optimization can add client-side or server-side compression if storage becomes an issue.

### Risk: Storyboard changes after audio generation
If a shot's narrative content changes after audio is generated, the audio becomes stale. Developers must manually trigger override mode or delete audio_file_path to regenerate. This is acceptable for MVP as narrative changes are rare after shot production completes.

## Migration Plan

### Database Migration
1. Create new migration: `000XXX_add_shots_audio_path.sql`
2. Add column: `ALTER TABLE public.shots ADD COLUMN audio_file_path TEXT`
3. Column is nullable initially; audio generation is opt-in
4. No data migration needed; existing shots remain functional without audio

### Code Rollout
1. Deploy database migration first
2. Deploy backend audio generation task and CLI
3. Run audio generation on existing stories using `--override` mode if needed
4. Deploy UI changes to display audio player controls (separate future work)

### Rollback Plan
If audio generation causes issues:
1. Stop running audio generation tasks
2. Remove audio_file_path column via down migration if needed
3. Audio files remain in public directory but are not referenced

## Open Questions
None remaining after user clarifications. All ambiguities resolved:
- narrator_voice_profile is at top level (user will update system prompt)
- voice_name and character_id fields will be added to profiles (user will update system prompt)
- Maximum 2 speakers total including narrator
- Use character_id for speaker names in Gemini API
